{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GlacierDelineationDeepLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridhar9800/Gesture-based-driving-using-OpenCV/blob/master/GlacierDelineationDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dzqG3bhaLGU7"
      },
      "source": [
        "This is a code to segment a Landsat ETM+ image in to glacier and non glacier binary image. The segmentation is performed using the U-Net convolutional Neural Network model trained using the Pytorch Library. \n",
        "Part of the code is based on the following implementation: Deep networks for Earth Observation (https://github.com/nshaud/DeepNetsForEO). I would like to hereby acknowledge that the above mentioned respository has helped me in getting starting with Deep Learning for remote sensing image segmentation using Pytorch. I am grateful to the authors for sharing their code. \n",
        "\n",
        "In order to run this code, a Google account is required. The Landsat ETM+ image is exported from the EarthEngine to the Drive and then feeded in to the neural network for prediction. The exporting of the image to the Google Drive may take 5-10 minutes.\n",
        "\n",
        "The False color original image as well as the predicted mask is then displayed on the map. \n",
        "\n",
        "The user needs to authorize access to the Google Drive below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u7r6sheJmYdl",
        "colab": {}
      },
      "source": [
        "!pip install pyproj   # pyproj is required for coordinate transformations\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torch.nn.functional as F\n",
        "from skimage import io\n",
        "from glob import glob\n",
        "import folium   # for visualizing maps and images\n",
        "import random\n",
        "import itertools\n",
        "import gdal\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms  \n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler\n",
        "import torch.nn.init\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pyproj import Proj, transform\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "    \n",
        "use_cuda = True\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a5pRKw-wawGA"
      },
      "source": [
        "As we are using Earth Engine libraries to read and export the Landsat image, one would need to authorize access for Earth Engine. Copy paste the code below for authorization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O75RRJRqmk-M",
        "colab": {}
      },
      "source": [
        "!pip install earthengine-api  # install the Earth Engine API\n",
        "!earthengine authenticate\n",
        "import ee\n",
        "ee.Initialize()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UgTa635xi2Vk"
      },
      "source": [
        "Below we select a Landsat ETM+ image. At the moment only a central region of the image is exported because exporting a full image takes quite long time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j1niJ1mguhZ8",
        "colab": {}
      },
      "source": [
        "image=ee.Image('LANDSAT/LE07/C01/T1/LE07_149035_20010930')  #change the name of this image if testing on another image is required\n",
        "projI=image.select('B1').projection().getInfo()    # Exporting the whole image takes time, therefore, roughly select the center region of the image\n",
        "crs=projI['crs']\n",
        "outProj=Proj(init='epsg:4326')\n",
        "inProj=Proj(init=crs)\n",
        "kk=projI['transform']\n",
        "print(projI)\n",
        "lon1, lat1=transform(inProj,outProj,kk[2]+(2000*30),kk[5]-(2000*30))\n",
        "lon2, lat2=transform(inProj,outProj,kk[2]+(5000*30),kk[5]-(5000*30)) #change these coordinates to increase or decrease image size\n",
        "\n",
        "bounds = [lon1, lat2, lon2, lat1]\n",
        "print(bounds,lon1,lat1)\n",
        "\n",
        "imageL7=imageL7.select(['B1','B2','B3','B4','B5','B6_VCID_2','B7','B8'])\n",
        "geometry = ([lon1, lat1],[lon1,lat2],[lon2,lat2],[lon2 ,lat1])\n",
        "config= {\n",
        "    'description':'Landsat07image',    \n",
        "    'region':geometry ,\n",
        "    'scale': 15,  #the image is exported with 15m resolution\n",
        "    'fileFormat': 'GeoTIFF',\n",
        "    'maxPixels':'1e12'\n",
        "}\n",
        "\n",
        "exp=ee.batch.Export.image.toDrive(imageL7,**config);\n",
        "\n",
        "exp.start()   # It takes around 5-10 minutes for 6000 * 6000 * 8 image to be exported \n",
        "print(exp.status())\n",
        "print(ee.batch.Task.list())\n",
        "import time\n",
        "while exp.active():\n",
        "  print('Transferring Data to Drive..................')\n",
        "  time.sleep(30)\n",
        "print('Done with the Export to the Drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7qcicdTqcY2f"
      },
      "source": [
        "This portion contains the network definition. At the moment I am taking GroupNorm equal to number of channels which should be equal to the instance Norm. There are four encoder and four decoder units. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PJr39RUXbnMw",
        "colab": {}
      },
      "source": [
        "class ConvBnRelu(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n",
        "        super(ConvBnRelu, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
        "        #self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.bn = nn.GroupNorm(in_channels,in_channels)\n",
        "       # self.relu = nn.ReLU()\n",
        "        self.relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.bn(x)\n",
        "        x = self.conv(x)       \n",
        "        x = self.relu(x)        \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class StackEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(StackEncoder, self).__init__()\n",
        "        self.convr1 = ConvBnRelu(in_channels, out_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.convr2 = ConvBnRelu(out_channels, out_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.maxPool = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convr1(x)\n",
        "        x = self.convr2(x)\n",
        "        \n",
        "        x_trace = x\n",
        "        x = self.maxPool(x)\n",
        "        return x, x_trace\n",
        "\n",
        "\n",
        "class StackDecoder(nn.Module):\n",
        "    def __init__(self, in_channels1,in_channels2, out_channels):\n",
        "        super(StackDecoder, self).__init__()\n",
        "\n",
        "        #self.upSample = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
        "        self.upSample = nn.ConvTranspose2d(in_channels1,in_channels1, (2,2), stride=2)\n",
        "        \n",
        "        self.convr1 = ConvBnRelu(in_channels1+in_channels2, out_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        self.convr2 = ConvBnRelu(out_channels, out_channels, kernel_size=(3, 3), stride=1, padding=1)\n",
        "    def _crop_concat(self, upsampled, bypass):\n",
        "        return torch.cat((upsampled, bypass), 1)\n",
        "\n",
        "    def forward(self, x, down_tensor):\n",
        "        x = self.upSample(x)\n",
        "        x = self._crop_concat(x, down_tensor)\n",
        "        x = self.convr1(x)    \n",
        "        x = self.convr2(x)\n",
        "       # x = self.convr3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNetOriginal(nn.Module):\n",
        "    @staticmethod\n",
        "    def weight_init(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            torch.nn.init.kaiming_normal(m.weight.data)\n",
        "\n",
        "    def __init__(self, in_shape):\n",
        "        super(UNetOriginal, self).__init__()\n",
        "        channels, height, width = in_shape\n",
        "\n",
        "        self.down1 = StackEncoder(channels,16)\n",
        "        self.down2 = StackEncoder(16, 16)\n",
        "        self.down3 = StackEncoder(16, 32)\n",
        "        self.down4 = StackEncoder(32,32)\n",
        "\n",
        "        self.center = nn.Sequential(\n",
        "            ConvBnRelu(32, 32, kernel_size=(3, 3), stride=1, padding=1),\n",
        "            ConvBnRelu(32, 32, kernel_size=(3, 3), stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "        self.up1 = StackDecoder(in_channels1=32,in_channels2=32, out_channels=32)\n",
        "        self.up2 = StackDecoder(in_channels1=32,in_channels2=32, out_channels=32)\n",
        "        self.up3 = StackDecoder(in_channels1=32,in_channels2=16, out_channels=16)\n",
        "        self.up4 = StackDecoder(in_channels1=16,in_channels2=16, out_channels=16)\n",
        "\n",
        "        self.output_seg_map = nn.Conv2d(16, 2, kernel_size=(1, 1), padding=0, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, x_trace1 = self.down1(x)  \n",
        "        x, x_trace2 = self.down2(x)\n",
        "        x, x_trace3 = self.down3(x)\n",
        "        x, x_trace4 = self.down4(x)\n",
        "\n",
        "        x = self.center(x)\n",
        "\n",
        "        x = self.up1(x, x_trace4)\n",
        "        x = self.up2(x, x_trace3)\n",
        "        x = self.up3(x, x_trace2)\n",
        "        x = self.up4(x, x_trace1)\n",
        "\n",
        "        out = self.output_seg_map(x)\n",
        "        out = torch.squeeze(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HpA76Tucjd4Z"
      },
      "source": [
        "The inference using the trained model is done here.  The \"UNet_Glaciers\" contains the trained model. The input image is imported from the Google Drive. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "93g7du4TEW2m",
        "colab": {}
      },
      "source": [
        "\n",
        "WINDOW_SIZE = (1024 ,1024) # Patch size\n",
        "IN_CHANNELS =8  # Use 8 bands of ETM+\n",
        "BATCH_SIZE =4  # Number of samples in a mini-batch\n",
        "\n",
        "LABELS = [\"Backgr\",\"Glaciers\"] # Label names\n",
        "N_CLASSES = len(LABELS) # Number of classes\n",
        "WEIGHTS = torch.ones(N_CLASSES) # Weights for class balancing\n",
        "CACHE = False # Store the dataset in-memory\n",
        "\n",
        "\n",
        "\n",
        "def CrossEntropy2d(input, target, weight=None, size_average=True):\n",
        "    \"\"\" 2D version of the cross entropy loss \"\"\"\n",
        "    dim = input.dim()\n",
        "    if dim == 2:\n",
        "        return F.cross_entropy(input, target, weight, size_average)\n",
        "    elif dim == 4:\n",
        "        output = input.view(input.size(0),input.size(1), -1)\n",
        "        output = torch.transpose(output,1,2).contiguous()\n",
        "        output = output.view(-1,output.size(2))\n",
        "        target = target.view(-1)\n",
        "        return F.cross_entropy(output, target,weight, size_average)\n",
        "    else:\n",
        "        raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))\n",
        "\n",
        "def accuracy(input, target):\n",
        "    return 100 * float(np.count_nonzero(input == target)) / target.size\n",
        "\n",
        "def sliding_window(top, step=10, window_size=(20,20)):\n",
        "    \"\"\" Slide a window_shape window across the image with a stride of step \"\"\"\n",
        "    for x in range(0, top.shape[0], step):\n",
        "        if x + window_size[0] > top.shape[0]:\n",
        "            x = top.shape[0] - window_size[0]\n",
        "        for y in range(0, top.shape[1], step):\n",
        "            if y + window_size[1] > top.shape[1]:\n",
        "                y = top.shape[1] - window_size[1]\n",
        "            yield x, y, window_size[0], window_size[1]\n",
        "            \n",
        "def count_sliding_window(top, step=10, window_size=(20,20)):\n",
        "    \"\"\" Count the number of windows in an image \"\"\"\n",
        "    c = 0\n",
        "    for x in range(0, top.shape[0], step):\n",
        "        if x + window_size[0] > top.shape[0]:\n",
        "            x = top.shape[0] - window_size[0]\n",
        "        for y in range(0, top.shape[1], step):\n",
        "            if y + window_size[1] > top.shape[1]:\n",
        "                y = top.shape[1] - window_size[1]\n",
        "            c += 1\n",
        "    return c\n",
        "  \n",
        "def grouper(n, iterable):\n",
        "    \"\"\" Browse an iterator by chunk of n elements \"\"\"\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        chunk = tuple(itertools.islice(it, n))\n",
        "        if not chunk:\n",
        "            return\n",
        "        yield chunk\n",
        "     \n",
        "\n",
        "def testOnly(net, test_ids, all=False, stride=WINDOW_SIZE[0], batch_size=BATCH_SIZE, window_size=WINDOW_SIZE):\n",
        "     \n",
        "    #test_files  = glob(test_Folder)\n",
        "    test_ids = list(range(1,len(test_files)+1))\n",
        "    for k in range(len(test_ids)):\n",
        "        \n",
        "           \n",
        "        test_images = (1 / 255 * np.asarray(io.imread(test_files[int(test_ids[k])-1]), dtype='float32') )\n",
        "        \n",
        "        all_preds = []\n",
        "\n",
        "        net.eval()\n",
        "   \n",
        "    \n",
        "        img=test_images\n",
        "        pred = np.zeros((img.shape[0],img.shape[1],N_CLASSES),)\n",
        "        gt = np.zeros((img.shape[0],img.shape[1]))\n",
        "        stride=256\n",
        "        total = count_sliding_window(gt, step=stride, window_size=window_size) // batch_size\n",
        "        for i, coords in enumerate(tqdm(grouper(batch_size, sliding_window(gt, step=stride, window_size=window_size)), total=total, leave=False)):\n",
        "            # Display in progress results\n",
        "                    \n",
        "            # Build the tensor\n",
        "            image_patches = [np.copy(img[x:x+w, y:y+h]).transpose((2,0,1)) for x,y,w,h in coords]\n",
        "            image_patches = np.asarray(image_patches)\n",
        "            image_patches = Variable(torch.from_numpy(image_patches).cuda(), volatile=True)\n",
        "            \n",
        "            # Do the inference\n",
        "            outs = net(image_patches)\n",
        "            outs = F.softmax(outs, dim=1)\n",
        "            outs = outs.data.cpu().numpy()\n",
        "            \n",
        "            # Fill in the results array\n",
        "            for out, (x, y, w, h) in zip(outs, coords):\n",
        "                out = out.transpose((1,2,0))\n",
        "                pred[x:x+w, y:y+h] += out\n",
        "            del(outs)\n",
        "\n",
        "        pred = np.argmax(pred, axis=-1)\n",
        "        \n",
        "        fig = plt.figure()\n",
        "        fig.add_subplot(1,2,1)\n",
        "        plt.imshow(np.asarray(255 * img[:,:,1], dtype='uint8'))\n",
        "        fig.add_subplot(1,2,2)\n",
        "        plt.imshow((pred))\n",
        "        plt.show()\n",
        "        return pred\n",
        "                                 \n",
        "      \n",
        "net = UNetOriginal((IN_CHANNELS,WINDOW_SIZE[0],WINDOW_SIZE[1]))      \n",
        "net.cuda()\n",
        "\n",
        "path = F\"/content/gdrive/My Drive/UNet_Glaciers\"\n",
        "net.load_state_dict(torch.load(path))\n",
        "\n",
        "# We define the scheduler. \n",
        "#base_lr = 0.01\n",
        "#optimizer = optim.SGD(net.parameters(), lr=base_lr, momentum=0.90, weight_decay=0.0005)\n",
        "#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [47,49], gamma=0.1)\n",
        "\n",
        "test_ids=[1]\n",
        "test_files  = glob(r'/content/gdrive/My Drive/Landsat07image.tif')\n",
        "print(test_files)\n",
        "pred = testOnly(net, test_ids, all=False, stride=min(WINDOW_SIZE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MADSir_SjOII"
      },
      "source": [
        "The predicted image along with the false color image is displayed over the openstreet map base layer using Folium library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G3Fo67HZ_l5I",
        "colab": {}
      },
      "source": [
        "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
        "bands=['B1','B2','B3','B4','B5','B6','B7','B8','B9']\n",
        "bounds1 = [[lat1,lon1],[lat2, lon2]]\n",
        "m=folium.Map(location=[ lat1,lon1],zoom_start=12)\n",
        "mapid=image.getMapId({'bands':['B5','B4','B3']})    \n",
        "folium.TileLayer(\n",
        "  tiles=EE_TILES.format(**mapid),\n",
        "  attr='Google Earth Engine',\n",
        "  overlay=True,\n",
        " ).add_to(m)  \n",
        "#m.add_child(folium.LayerControl())\n",
        "\n",
        "\n",
        "img = folium.raster_layers.ImageOverlay(\n",
        "  name='PredictedImage',\n",
        "  image=pred,\n",
        "  bounds=bounds1,\n",
        "  interactive=True,\n",
        "  overlay=True,\n",
        ")\n",
        "img.add_to(m)\n",
        "m.add_child(folium.LayerControl())\n",
        "m"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}